{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Demand forecasting with BigQuery and TensorFlow</h1>\n",
    "\n",
    "In this notebook, we will develop a machine learning model to predict the demand for taxi cabs in New York.\n",
    "\n",
    "To develop the model, we will need to get historical data of taxicab usage. This data exists in BigQuery. Let's start by looking at the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datalab.bigquery as bq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%projects set drwgry-proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bqsv\" id=\"1_148929098163\"></div>\n",
       "    <script>\n",
       "      require.config({\n",
       "        map: {\n",
       "          '*': {\n",
       "            datalab: 'nbextensions/gcpdatalab'\n",
       "          }\n",
       "        },\n",
       "      });\n",
       "\n",
       "      require(['datalab/bigquery', 'datalab/element!1_148929098163',\n",
       "          'datalab/style!/nbextensions/gcpdatalab/bigquery.css'],\n",
       "        function(bq, dom) {\n",
       "          bq.renderSchema(dom, [{\"type\": \"TIMESTAMP\", \"name\": \"pickup_datetime\", \"mode\": \"NULLABLE\"}, {\"type\": \"TIMESTAMP\", \"name\": \"dropoff_datetime\", \"mode\": \"NULLABLE\"}, {\"type\": \"STRING\", \"name\": \"store_and_fwd_flag\", \"mode\": \"NULLABLE\"}, {\"type\": \"INTEGER\", \"name\": \"rate_code\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"pickup_longitude\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"pickup_latitude\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"dropoff_longitude\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"dropoff_latitude\", \"mode\": \"NULLABLE\"}, {\"type\": \"INTEGER\", \"name\": \"passenger_count\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"trip_distance\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"fare_amount\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"extra\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"mta_tax\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"tip_amount\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"tolls_amount\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"ehail_fee\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"total_amount\", \"mode\": \"NULLABLE\"}, {\"type\": \"INTEGER\", \"name\": \"payment_type\", \"mode\": \"NULLABLE\"}, {\"type\": \"FLOAT\", \"name\": \"distance_between_service\", \"mode\": \"NULLABLE\"}, {\"type\": \"INTEGER\", \"name\": \"time_between_service\", \"mode\": \"NULLABLE\"}, {\"type\": \"INTEGER\", \"name\": \"trip_type\", \"mode\": \"NULLABLE\"}]);\n",
       "        }\n",
       "      );\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%bigquery schema --table \"nyc-tlc:green.trips_2015\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Analyzing taxicab demand </h2>\n",
    "\n",
    "Let's pull the number of trips for each day in the 2015 dataset.  We can use the BigQuery built-in Date-and-time function DAYOFYEAR (See <a href=\"https://cloud.google.com/bigquery/query-reference\"> BigQuery query reference </a>) for a full list of such functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No application credentials found. Perhaps you should sign in.\n"
     ]
    }
   ],
   "source": [
    "%sql\n",
    "SELECT DAYOFYEAR(pickup_datetime) AS daynumber FROM [nyc-tlc:green.trips_2015] LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Modular queries and Pandas dataframe </h3>\n",
    "\n",
    "Let's use the total number of trips as our proxy for taxicab demand (other reasonable alternatives are total trip_distance or total fare_amount).  It is possible to predict multiple variables using Tensorflow, but for simplicity, we will stick to just predicting the number of trips.\n",
    "\n",
    "We will give our query a name 'taxiquery' and have it use an input variable '$YEAR'. We can then invoke the 'taxiquery' by giving it a YEAR.  The to_dataframe() converts the BigQuery result into a <a href='http://pandas.pydata.org/'>Pandas</a> dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module taxiquery\n",
    "SELECT daynumber, COUNT(*) AS numtrips FROM\n",
    "    (SELECT DAYOFYEAR(pickup_datetime) AS daynumber FROM [nyc-tlc:green.trips_$YEAR])\n",
    "GROUP BY daynumber ORDER BY daynumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips = bq.Query(taxiquery, YEAR=2015).to_dataframe()\n",
    "trips[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Benchmark </h3>\n",
    "\n",
    "Often, a reasonable estimate of something is its historical average. We can therefore benchmark our machine learning model against the historical average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg = np.mean(trips['numtrips'])\n",
    "print 'Just using average={0} has RMSE of {1}'.format(avg, np.sqrt(np.mean((trips['numtrips'] - avg)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean here is about 55,000 and the root-mean-square-error (RMSE) in this case is about 10,000. In other words, if we were to estimate that there are 55,000 taxi trips on any given day, that estimate is will be off on average by about 10,000 in either direction.\n",
    "  \n",
    "Let's see if we can do better than this -- our goal is to make predictions of taxicab demand whose RMSE is lower than 10,000.\n",
    "\n",
    "What kinds of things affect people's use of taxicabs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Weather data </h2>\n",
    "\n",
    "We suspect that weather influences how often people use a taxi. Perhaps someone who'd normally walk to work would take a taxi if it is very cold or rainy.\n",
    "\n",
    "Googler <a href=\"https://twitter.com/felipehoffa\">Felipe Hoffa</a> has made weather observations from the US National Oceanic and Atmospheric Administration <a href-=\"http://stackoverflow.com/questions/34804654/how-to-get-the-historical-weather-for-any-city-with-bigquery/34804655\">publicly</a> available in BigQuery. Let's use that dataset and find the station number corresponding to New York's La Guardia airport.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM [fh-bigquery:weather_gsod.stations]\n",
    "WHERE state = 'NY' AND wban != '99999' AND name contains 'LA GUARDIA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Variables </h3>\n",
    "\n",
    "Let's pull out the minimum and maximum daily temperature (in Fahrenheit) as well as the amount of rain (in inches) for La Guardia airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module wxquery\n",
    "SELECT DAYOFYEAR(TIMESTAMP('$YEAR'+mo+da)) daynumber,\n",
    "       FIRST(DAYOFWEEK(TIMESTAMP('$YEAR'+mo+da))) dayofweek,\n",
    "       MIN(min) mintemp, MAX(max) maxtemp, MAX(IF(prcp=99.99,0,prcp)) rain\n",
    "FROM [fh-bigquery:weather_gsod.gsod$YEAR]\n",
    "WHERE stn='725030' GROUP BY 1 ORDER BY daynumber DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather = bq.Query(wxquery, YEAR=2015).to_dataframe()\n",
    "weather[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Merge datasets </h3>\n",
    "\n",
    "Let's use Pandas to merge (combine) the taxi cab and weather datasets day-by-day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.merge(weather, trips, on='daynumber')\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exploratory analysis </h3>\n",
    "\n",
    "Is there a relationship between maximum temperature and the number of trips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = data.plot(kind='scatter', x='maxtemp', y='numtrips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot above doesn't look very promising. \n",
    "\n",
    "Is there a relationship between the day of the week and the number of trips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = data.plot(kind='scatter', x='dayofweek', y='numtrips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurrah, we seem to have found a predictor. It appears that people use taxis more later in the week. Perhaps New Yorkers make weekly resolutions to walk more and then lose their determination later in the week, or maybe it reflects tourism dynamics in New York City.\n",
    "\n",
    "Perhaps if we took out the <em>confounding</em> effect of the day of the week, maximum temperature will start to have an effect. Let's see if that's the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = data[data['dayofweek'] == 7].plot(kind='scatter', x='maxtemp', y='numtrips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the confounding factor does seem to reflect an underlying trend around temperature. But ... the data are a little sparse, don't you think?  This is something that you have to keep in mind -- the more predictors you start to consider (here we are using two: day of week and maximum temperature), the more rows you will need so as to avoid <em> overfitting </em> the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Adding 2014 data </h3>\n",
    "\n",
    "Let's add in 2014 data to the Pandas dataframe.  Note how useful it was for us to modularize our queries around the YEAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trips = bq.Query(taxiquery, YEAR=2014).to_dataframe()\n",
    "weather = bq.Query(wxquery, YEAR=2014).to_dataframe()\n",
    "data2014 = pd.merge(weather, trips, on='daynumber')\n",
    "data2014[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = pd.concat([data, data2014])\n",
    "data2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = data2[data2['dayofweek'] == 7].plot(kind='scatter', x='maxtemp', y='numtrips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data do seem a bit more robust.  If we had more data, it would be better of course. But in this case, we only have 2014 and 2015 data, so that's what we will go with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Machine Learning with Tensorflow </h2>\n",
    "\n",
    "We'll use 80% of our dataset for training and 20% of the data for testing the model we have trained. Let's shuffle the rows of the Pandas dataframe so that this division is random.  The predictor (or input) columns will be every column in the database other than the number-of-trips (which is our target, or what we want to predict).\n",
    "\n",
    "The machine learning models that we will use -- linear regression and neural networks -- both require that the input variables are numeric in nature.\n",
    "\n",
    "The day of the week, however, is a categorical variable (i.e. Tuesday is not really greater than Monday). So, we should create separate columns for whether it is a Monday (with values 0 or 1), Tuesday, etc.\n",
    "\n",
    "Against that, we do have limited data (remember: the more columns you use as input features, the more rows you need to have in your training dataset), and it appears that there is a clear linear trend by day of the week. So, we will opt for simplicity here and use the data as-is.  Try uncommenting the code that creates separate columns for the days of the week and re-run the notebook if you are curious about the impact of this simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "shuffled = data2.sample(frac=1)\n",
    "# It would be a good idea, if we had more data, to treat the days as categorical variables\n",
    "# with the small amount of data, we have though, the model tends to overfit\n",
    "#predictors = shuffled.iloc[:,2:5]\n",
    "#for day in xrange(1,8):\n",
    "#  matching = shuffled['dayofweek'] == day\n",
    "#  predictors.loc[matching, 'day_' + str(day)] = 1\n",
    "#  predictors.loc[~matching, 'day_' + str(day)] = 0\n",
    "predictors = shuffled.iloc[:,1:5]\n",
    "predictors[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = shuffled.iloc[:,5]\n",
    "targets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our benchmark based on the 80-20 split and the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainsize = int(len(shuffled['numtrips']) * 0.8)\n",
    "avg = np.mean(shuffled['numtrips'][:trainsize])\n",
    "rmse = np.sqrt(np.mean((targets[trainsize:] - avg)**2))\n",
    "print 'Just using average={0} has RMSE of {1}'.format(avg, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Linear regression with tf.contrib.learn </h2>\n",
    "\n",
    "We scale the number of taxicab rides by 100,000 so that the model can keep its predicted values in the [0-1] range. The optimization goes a lot faster when the weights are small numbers.  We save the weights into /tmp/trained_model and display the root mean square error on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SCALE_NUM_TRIPS = 100000.0\n",
    "trainsize = int(len(shuffled['numtrips']) * 0.8)\n",
    "testsize = len(shuffled['numtrips']) - trainsize\n",
    "npredictors = len(predictors.columns)\n",
    "noutputs = 1\n",
    "tf.logging.set_verbosity(tf.logging.WARN) # change to INFO to get output every 100 steps ...\n",
    "shutil.rmtree('/tmp/trained_model', ignore_errors=True) # so that we don't load weights from previous runs\n",
    "estimator = tf.contrib.learn.LinearRegressor(model_dir='/tmp/trained_model',\n",
    "                                             optimizer=tf.train.AdamOptimizer(learning_rate=0.1),\n",
    "                                             enable_centered_bias=False,\n",
    "                                             feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values))\n",
    "print \"starting to train ... this will take a while ... use verbosity=INFO to get more verbose output\"\n",
    "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_NUM_TRIPS, steps=10000)\n",
    "pred = np.multiply(list(estimator.predict(predictors[trainsize:].values)), SCALE_NUM_TRIPS )\n",
    "rmse = np.sqrt(np.mean(np.power((targets[trainsize:].values - pred), 2)))\n",
    "print 'LinearRegression has RMSE of {0}'.format(rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error of about 11420 indicates that we are doing better with the machine learning model than we would be if we were to just use the historical average (our benchmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neural network with tf.contrib.learn </h2>\n",
    "\n",
    "Let's make a more complex model with a few hidden nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SCALE_NUM_TRIPS = 100000.0\n",
    "trainsize = int(len(shuffled['numtrips']) * 0.8)\n",
    "testsize = len(shuffled['numtrips']) - trainsize\n",
    "npredictors = len(predictors.columns)\n",
    "noutputs = 1\n",
    "tf.logging.set_verbosity(tf.logging.WARN) # change to INFO to get output every 100 steps ...\n",
    "shutil.rmtree('/tmp/trained_model', ignore_errors=True) # so that we don't load weights from previous runs\n",
    "estimator = tf.contrib.learn.DNNRegressor(model_dir='/tmp/trained_model',\n",
    "                                          hidden_units=[5, 2],\n",
    "                                          optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "                                          enable_centered_bias=False,\n",
    "                                          feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(predictors.values))\n",
    "print \"starting to train ... this will take a while ... use verbosity=INFO to get more verbose output\"\n",
    "estimator.fit(predictors[:trainsize].values, targets[:trainsize].values.reshape(trainsize, noutputs)/SCALE_NUM_TRIPS, steps=10000)\n",
    "pred = estimator.predict(predictors[trainsize:].values) * SCALE_NUM_TRIPS\n",
    "rmse = np.sqrt(np.mean((targets[trainsize:].values - pred)**2))\n",
    "print 'Neural Network Regression has RMSE of {0}'.format(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a neural network does seem to get us an additional boost in performance (11400 to 10600 when I ran it -- because of random seeds, your actual answers might be different)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running a trained model </h2>\n",
    "\n",
    "So, we have trained a model, and saved it to a file. Let's use this model to predict taxicab demand given the expected weather for three days.\n",
    "\n",
    "Here we make a Dataframe out of those inputs, load up the saved model (note that we have to know the model equation -- it's not saved in the model file) and use it to predict the taxicab demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = pd.DataFrame.from_dict(data = \n",
    "                               {'dayofweek' : [4, 5, 6],\n",
    "                                'mintemp' : [60, 30, 70],\n",
    "                                'maxtemp' : [80, 70, 80],\n",
    "                                'rain' : [0, 0.8, 0]})\n",
    "# read trained model from /tmp/trained_model\n",
    "estimator = tf.contrib.learn.DNNRegressor(model_dir='/tmp/trained_model',\n",
    "                                          hidden_units=[5, 2],\n",
    "                                          enable_centered_bias=False,\n",
    "                                          feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(input.values))\n",
    "\n",
    "pred = estimator.predict(input.values) * SCALE_NUM_TRIPS\n",
    "print pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we should tell some of our taxi drivers to take the day off on Wednesday (day=4) and be there in full strength on Thursday (day=5).  No wonder -- the forecast calls for extreme weather fluctuations on Thursday.\n",
    "\n",
    "Note that Thursdays are usually \"slow\" days (taxi demand peaks on the weekends), but the machine learning model tells us to expect heavy demand this particular Thursday because of the weather."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
